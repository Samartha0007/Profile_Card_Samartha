<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Samarth | Duo Voice Assistant</title>
  <!-- Linking Google Fonts For Icons -->
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Rounded:opsz,wght,FILL,GRAD@24,400,0,0" />
  <link rel="shortcut icon" href="https://i.ibb.co/2ZrGMjH/icon.jpg" type="image/webp">
  <link rel="icon" href="https://i.ibb.co/JkJPs03/images-2.jpg" type="image/webp">
  <style>
    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
      font-family: 'Inter', sans-serif;
    }
    
    body {
      background: #f0f2f5;
      height: 100vh;
      display: flex;
      flex-direction: column;
      justify-content: space-between;
    }
    
    .header {
      padding: 20px;
      text-align: center;
      background: linear-gradient(145deg, #3454d1, #34d1bf);
      color: #fff;
      border-radius: 0 0 20px 20px;
      box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1);
      transition: all 0.3s ease;
    }
    
    .title {
      font-size: 2rem;
      margin-bottom: 8px;
    }
    
    .subtitle {
      font-size: 1rem;
      opacity: 0.9;
    }
    
    .call-interface {
      flex: 1;
      display: flex;
      flex-direction: column;
      justify-content: space-between;
      padding: 20px;
      overflow-y: auto;
    }
    
    .call-status {
      text-align: center;
      padding: 15px;
      margin: 20px auto;
      background: #fff;
      border-radius: 15px;
      box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
      width: 80%;
      max-width: 400px;
      transition: all 0.3s ease;
    }
    
    .status-text {
      font-size: 1.2rem;
      color: #333;
      margin-bottom: 10px;
    }
    
    .transcript {
      background: #fff;
      border-radius: 15px;
      padding: 15px;
      margin-top: 20px;
      box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
      flex: 1;
      overflow-y: auto;
      margin-bottom: 20px;
    }
    
    .speech-bubble {
      padding: 12px 15px;
      margin: 10px 0;
      border-radius: 18px;
      max-width: 80%;
      animation: fadeIn 0.3s ease;
    }
    
    .user-speech {
      background: #e1f5fe;
      margin-left: auto;
      color: #01579b;
      border-top-right-radius: 4px;
    }
    
    .assistant-speech {
      background: #e8f5e9;
      margin-right: auto;
      color: #2e7d32;
      border-top-left-radius: 4px;
    }
    
    .controls {
      display: flex;
      justify-content: center;
      align-items: center;
      padding: 20px;
      background: #fff;
      border-radius: 20px 20px 0 0;
      box-shadow: 0 -2px 10px rgba(0, 0, 0, 0.1);
      position: relative;
    }
    
    .mic-button {
      width: 70px;
      height: 70px;
      background: linear-gradient(145deg, #3454d1, #34d1bf);
      border: none;
      border-radius: 50%;
      display: flex;
      justify-content: center;
      align-items: center;
      cursor: pointer;
      box-shadow: 0 4px 15px rgba(0, 0, 0, 0.2);
      transition: all 0.2s ease;
      margin: 0 15px;
    }
    
    .setup-voice-btn {
      position: absolute;
      right: 20px;
      width: 40px;
      height: 40px;
      background: #f0f2f5;
      border: none;
      border-radius: 50%;
      display: flex;
      justify-content: center;
      align-items: center;
      cursor: pointer;
      box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
      transition: all 0.2s ease;
    }
    
    .mic-button:hover {
      transform: scale(1.05);
    }
    
    .mic-button:active {
      transform: scale(0.95);
    }
    
    .mic-icon {
      font-size: 32px;
      color: #fff;
    }
    
    .pulsating {
      animation: pulse 1.5s infinite;
    }
    
    .typing-indicator {
      display: inline-block;
      margin-left: 5px;
    }
    
    .typing-indicator span {
      display: inline-block;
      width: 8px;
      height: 8px;
      background-color: #2e7d32;
      border-radius: 50%;
      margin: 0 2px;
      animation: typing 1.5s infinite;
    }
    
    .typing-indicator span:nth-child(2) {
      animation-delay: 0.2s;
    }
    
    .typing-indicator span:nth-child(3) {
      animation-delay: 0.4s;
    }
    
    .disclaimer-text {
      text-align: center;
      font-size: 0.8rem;
      color: #777;
      margin-top: 10px;
    }
    
    @keyframes fadeIn {
      from { opacity: 0; transform: translateY(10px); }
      to { opacity: 1; transform: translateY(0); }
    }
    
    @keyframes pulse {
      0% { box-shadow: 0 0 0 0 rgba(52, 84, 209, 0.7); }
      70% { box-shadow: 0 0 0 15px rgba(52, 84, 209, 0); }
      100% { box-shadow: 0 0 0 0 rgba(52, 84, 209, 0); }
    }
    
    @keyframes typing {
      0%, 100% { transform: translateY(0); }
      50% { transform: translateY(-5px); }
    }
    
    .hide {
      display: none;
    }
    
    .listening-animation {
      width: 100%;
      height: 60px;
      display: flex;
      justify-content: center;
      align-items: center;
      gap: 5px;
    }
    
    .listening-bar {
      width: 6px;
      background: linear-gradient(145deg, #3454d1, #34d1bf);
      height: 15px;
      border-radius: 3px;
    }
    
    .listening .listening-bar {
      animation: soundBars 1.2s infinite;
    }
    
    .listening-bar:nth-child(2) { animation-delay: 0.2s; }
    .listening-bar:nth-child(3) { animation-delay: 0.4s; }
    .listening-bar:nth-child(4) { animation-delay: 0.6s; }
    .listening-bar:nth-child(5) { animation-delay: 0.8s; }
    
    @keyframes soundBars {
      0% { height: 15px; }
      50% { height: 45px; }
      100% { height: 15px; }
    }
    /* Voice Toggle Switch */
    .voice-toggle {
      margin-top: 10px;
      display: flex;
      align-items: center;
      justify-content: center;
    }
    
    .toggle-label {
      margin-left: 10px;
      color: #fff;
      font-size: 0.9rem;
    }
    
    .switch {
      position: relative;
      display: inline-block;
      width: 50px;
      height: 24px;
    }
    
    .switch input { 
      opacity: 0;
      width: 0;
      height: 0;
    }
    
    .slider {
      position: absolute;
      cursor: pointer;
      top: 0;
      left: 0;
      right: 0;
      bottom: 0;
      background-color: #ccc;
      transition: .4s;
    }
    
    .slider:before {
      position: absolute;
      content: "";
      height: 16px;
      width: 16px;
      left: 4px;
      bottom: 4px;
      background-color: white;
      transition: .4s;
    }
    
    input:checked + .slider {
      background-color: #2196F3;
    }
    
    input:focus + .slider {
      box-shadow: 0 0 1px #2196F3;
    }
    
    input:checked + .slider:before {
      transform: translateX(26px);
    }
    
    .slider.round {
      border-radius: 24px;
    }
    
    .slider.round:before {
      border-radius: 50%;
    }
    
    /* Voice Recording Setup Modal */
    .setup-modal {
      display: none;
      position: fixed;
      z-index: 10;
      left: 0;
      top: 0;
      width: 100%;
      height: 100%;
      background-color: rgba(0,0,0,0.6);
      align-items: center;
      justify-content: center;
    }
    
    .setup-modal.show {
      display: flex;
    }
    
    .modal-content {
      background-color: #fff;
      margin: auto;
      padding: 20px;
      border-radius: 15px;
      width: 80%;
      max-width: 500px;
      box-shadow: 0 5px 15px rgba(0,0,0,0.3);
    }
    
    .modal-header {
      display: flex;
      justify-content: space-between;
      align-items: center;
      margin-bottom: 15px;
    }
    
    .modal-title {
      font-size: 1.3rem;
      font-weight: bold;
      color: #333;
    }
    
    .close-btn {
      font-size: 1.5rem;
      cursor: pointer;
      color: #777;
    }
    
    .sample-text {
      padding: 15px;
      background-color: #f5f5f5;
      border-radius: 10px;
      margin: 15px 0;
      font-style: italic;
      color: #555;
    }
    
    .record-btn {
      background: linear-gradient(145deg, #ff4d67, #ff6b45);
      border: none;
      color: white;
      padding: 10px 15px;
      border-radius: 20px;
      font-size: 1rem;
      cursor: pointer;
      display: flex;
      align-items: center;
      margin: 0 auto;
    }
    
    .record-btn .material-symbols-rounded {
      margin-right: 5px;
    }
    
    .samples-list {
      margin-top: 15px;
    }
    
    .sample-item {
      display: flex;
      justify-content: space-between;
      align-items: center;
      padding: 8px 10px;
      background-color: #f0f0f0;
      border-radius: 8px;
      margin-bottom: 8px;
    }
    
    .save-voice-btn {
      background: linear-gradient(145deg, #3454d1, #34d1bf);
      border: none;
      color: white;
      padding: 10px 20px;
      border-radius: 20px;
      font-size: 1rem;
      cursor: pointer;
      margin-top: 15px;
      width: 100%;
    }
  </style>
</head>
<body>
  <header class="header">
    <!-- Header Greetings -->
    <h1 class="title">Duo Voice Assistant</h1>
    <p class="subtitle">Tap the mic and start speaking</p>
    <div class="voice-toggle">
      <label class="switch">
        <input type="checkbox" id="voiceToggle" checked>
        <span class="slider round"></span>
      </label>
      <span class="toggle-label">My Voice</span>
    </div>
  </header>

  <div class="call-interface">
    <div class="call-status">
      <p class="status-text">Ready to listen</p>
      <div class="listening-animation">
        <div class="listening-bar"></div>
        <div class="listening-bar"></div>
        <div class="listening-bar"></div>
        <div class="listening-bar"></div>
        <div class="listening-bar"></div>
      </div>
    </div>

    <div class="transcript"></div>
  </div>

  <div class="controls">
    <button class="mic-button">
      <span class="mic-icon material-symbols-rounded">mic</span>
    </button>
    <button class="setup-voice-btn" id="setupVoiceBtn">
      <span class="material-symbols-rounded">settings_voice</span>
    </button>
    <p class="disclaimer-text">
      Made with ❤️‍🔥 by Samartha Gs
    </p>
  </div>
  
  <!-- Voice Setup Modal -->
  <div class="setup-modal" id="voiceSetupModal">
    <div class="modal-content">
      <div class="modal-header">
        <div class="modal-title">Set Up Your Voice</div>
        <span class="close-btn">&times;</span>
      </div>
      <p>Record samples of your voice to personalize Duo's responses.</p>
      
      <div class="sample-text">
        "Hello, I'm your voice assistant. How can I help you today?"
      </div>
      
      <button class="record-btn">
        <span class="material-symbols-rounded">fiber_manual_record</span>
        Record Sample
      </button>
      
      <div class="samples-list">
        <div class="sample-item">
          <span>Sample 1</span>
          <span class="material-symbols-rounded">play_arrow</span>
        </div>
      </div>
      
      <button class="save-voice-btn">Save My Voice</button>
    </div>
  </div>

  <script>
    const micButton = document.querySelector(".mic-button");
    const setupVoiceBtn = document.getElementById("setupVoiceBtn");
    const voiceToggle = document.getElementById("voiceToggle");
    const setupModal = document.getElementById("voiceSetupModal");
    const closeModalBtn = document.querySelector(".close-btn");
    const recordBtn = document.querySelector(".record-btn");
    const saveVoiceBtn = document.querySelector(".save-voice-btn");
    const statusText = document.querySelector(".status-text");
    const listeningAnimation = document.querySelector(".listening-animation");
    const transcript = document.querySelector(".transcript");

    let recognition;
    let isSpeaking = false;
    let speechSynthesis = window.speechSynthesis;
    let mediaRecorder;
    let audioChunks = [];
    
    // Custom voice settings
    let customVoiceEnabled = true; // Set to true to use custom voice
    let voiceSamples = []; // Store voice sample data
    let hasRecordedVoice = false;

    const API_KEY = "AIzaSyAGt-0uYeWcj1olFe-yzbmbmW3R9k8Jmb8";
    const API_URL = `https://generativelanguage.googleapis.com/v1/models/gemini-1.5-pro:generateContent?key=${API_KEY}`;

    const predefinedResponses = {
      "who are you": "I am Duo, your personal voice assistant created by Samartha Gs. I'm here to help you with information and conversation.",
      "who developed you": "I was developed by Samartha Gs, a 17-year-old tech enthusiast who loves web development, IoT, and AI. He has completed over 50 amazing projects!",
      "where are you from": "I am from Golagodu, a beautiful village near Sagara in Karnataka, India.",
      "what can you do": "I can answer your questions, provide information, and chat about various topics through voice. Just speak, and I'll respond.",
      "who is samartha gs": "Samartha Gs is a talented 17-year-old passionate about web development, IoT, and AI. He has completed over 50 projects, including websites and apps.",
      "do you know samartha gs": "Yes, Samartha Gs is my creator! A skilled individual passionate about technology and innovation.",
      "samartha gs": "Samartha Gs - I am here because of him!",
      "hi": "Hi, I'm Duo-Developed by Samartha, How can I help you today?",
      "hello": "Hello, I'm Duo-Developed by Samartha, How can I help you today?",
    };

    // Initialize speech recognition
    if ('SpeechRecognition' in window || 'webkitSpeechRecognition' in window) {
      recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
      recognition.continuous = false;
      recognition.interimResults = true;
      recognition.lang = 'en-US';

      recognition.onstart = () => {
        statusText.textContent = "Listening...";
        listeningAnimation.classList.add("listening");
        micButton.classList.add("pulsating");
      };

      recognition.onresult = (event) => {
        const transcript_text = Array.from(event.results)
          .map(result => result[0].transcript)
          .join("");
        
        if (event.results[0].isFinal) {
          addUserSpeechBubble(transcript_text);
          processUserSpeech(transcript_text);
        }
      };

      recognition.onend = () => {
        if (!isSpeaking) {
          statusText.textContent = "Ready to listen";
          listeningAnimation.classList.remove("listening");
          micButton.classList.remove("pulsating");
        }
      };

      recognition.onerror = (event) => {
        console.error("Speech recognition error", event.error);
        statusText.textContent = "Error: " + event.error;
        listeningAnimation.classList.remove("listening");
        micButton.classList.remove("pulsating");
      };
    } else {
      alert("Speech recognition is not supported in your browser.");
      statusText.textContent = "Speech recognition not supported";
    }

    // Find predefined response based on similarity
    const findPredefinedResponse = (message) => {
      // Normalize the message
      message = message.toLowerCase();

      // Common prefixes to ignore
      const commonPrefixes = [
        "tell me about",
        "can you explain",
        "what do you know about",
        "do you know about",
        "i want to know about",
      ];

      // Remove common prefixes from the user message
      for (const prefix of commonPrefixes) {
        if (message.startsWith(prefix)) {
          message = message.replace(prefix, "").trim();
          break;
        }
      }

      const calculateSimilarity = (a, b) => {
        const wordsA = new Set(a.split(" "));
        const wordsB = new Set(b.split(" "));
        const intersection = new Set([...wordsA].filter((word) => wordsB.has(word)));
        return intersection.size / Math.max(wordsA.size, wordsB.size);
      };

      const threshold = 0.5; // Minimum similarity threshold (0 to 1)
      let bestMatch = null;
      let highestSimilarity = 0;

      // Check each predefined response for similarity
      for (const key in predefinedResponses) {
        const similarity = calculateSimilarity(message, key);
        if (similarity > highestSimilarity && similarity >= threshold) {
          highestSimilarity = similarity;
          bestMatch = predefinedResponses[key];
        }
      }

      return bestMatch;
    };

    // Process user speech
    const processUserSpeech = async (userSpeech) => {
      if (!userSpeech) return;

      showAssistantTyping();

      // Check for predefined response
      const predefinedResponse = findPredefinedResponse(userSpeech);
      
      if (predefinedResponse) {
        setTimeout(() => {
          // Create an audio feedback that sounds like your voice
          const responseText = prepareVoiceResponse(predefinedResponse, userSpeech);
          speakResponse(responseText);
        }, 1000);
      } else {
        try {
          const response = await fetch(API_
    };

    // Show typing indicator
    const showAssistantTyping = () => {
      const typingBubble = document.createElement("div");
      typingBubble.classList.add("speech-bubble", "assistant-speech", "typing");
      typingBubble.innerHTML = "Duo is thinking<div class='typing-indicator'><span></span><span></span><span></span></div>";
      transcript.appendChild(typingBubble);
      transcript.scrollTop = transcript.scrollHeight;
      return typingBubble;
    };

    // Add user speech bubble to transcript
    const addUserSpeechBubble = (text) => {
      const userBubble = document.createElement("div");
      userBubble.classList.add("speech-bubble", "user-speech");
      userBubble.textContent = text;
      transcript.appendChild(userBubble);
      transcript.scrollTop = transcript.scrollHeight;
    };

    // Add assistant speech bubble to transcript
    const addAssistantSpeechBubble = (text) => {
      // Remove any typing indicators
      const typingIndicators = transcript.querySelectorAll(".typing");
      typingIndicators.forEach(indicator => indicator.remove());
      
      const assistantBubble = document.createElement("div");
      assistantBubble.classList.add("speech-bubble", "assistant-speech");
      assistantBubble.textContent = text;
      transcript.appendChild(assistantBubble);
      transcript.scrollTop = transcript.scrollHeight;
    };

    // Speak response using custom voice (your voice)
    const speakResponse = (text) => {
      addAssistantSpeechBubble(text);
      
      isSpeaking = true;
      statusText.textContent = "Duo is speaking...";
      
      // Use custom voice API or stored audio
      if (customVoiceEnabled && voiceSamples.length > 0) {
        // Play custom voice audio
        playCustomVoiceResponse(text);
      } else {
        // Fallback to regular speech synthesis
        const utterance = new SpeechSynthesisUtterance(text);
        utterance.rate = 1.0;
        utterance.pitch = 1.0;
        utterance.volume = 1.0;
        
        // Select a voice (optional)
        const voices = speechSynthesis.getVoices();
        const preferredVoice = voices.find(voice => voice.lang.includes('en-US'));
        if (preferredVoice) {
          utterance.voice = preferredVoice;
        }
        
        // When speech ends
        utterance.onend = () => {
          isSpeaking = false;
          statusText.textContent = "Ready to listen";
        };
        
        // Speak
        speechSynthesis.speak(utterance);
      }
    };
    
    // Play response with custom voice
    const playCustomVoiceResponse = (text) => {
      // This would typically connect to a voice cloning API
      // For now, we'll simulate with pre-recorded samples
      console.log("Playing response with custom voice:", text);
      
      // Simple simulation of voice response timing
      const wordsPerMinute = 150;
      const words = text.split(' ').length;
      const durationMs = (words / wordsPerMinute) * 60 * 1000;
      
      // Play your custom voice audio here
      // This is where you would call your voice API or play stored samples
      
      // Simulate speech ending after appropriate duration
      setTimeout(() => {
        isSpeaking = false;
        statusText.textContent = "Ready to listen";
      }, durationMs);
    };

    // Handle microphone button click
    micButton.addEventListener("click", () => {
      if (speechSynthesis.speaking) {
        speechSynthesis.cancel();
        isSpeaking = false;
        statusText.textContent = "Ready to listen";
      } else if (recognition) {
        recognition.start();
      }
    });
    
    // Voice toggle change
    voiceToggle.addEventListener("change", () => {
      customVoiceEnabled = voiceToggle.checked;
      
      if (customVoiceEnabled && !hasRecordedVoice) {
        // Show setup modal if custom voice is enabled but not recorded yet
        setupModal.classList.add("show");
      }
    });
    
    // Setup voice button click
    setupVoiceBtn.addEventListener("click", () => {
      setupModal.classList.add("show");
    });
    
    // Close modal
    closeModalBtn.addEventListener("click", () => {
      setupModal.classList.remove("show");
    });
    
    // Record voice sample
    recordBtn.addEventListener("click", () => {
      if (!mediaRecorder || mediaRecorder.state === "inactive") {
        // Start recording
        startRecording();
        recordBtn.innerHTML = '<span class="material-symbols-rounded">stop</span> Stop Recording';
      } else {
        // Stop recording
        stopRecording();
        recordBtn.innerHTML = '<span class="material-symbols-rounded">fiber_manual_record</span> Record Sample';
      }
    });
    
    // Save voice settings
    saveVoiceBtn.addEventListener("click", () => {
      hasRecordedVoice = true;
      setupModal.classList.remove("show");
      statusText.textContent = "Custom voice activated";
      setTimeout(() => {
        statusText.textContent = "Ready to listen";
      }, 2000);
    });
    
    // Start voice recording
    const startRecording = async () => {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        mediaRecorder = new MediaRecorder(stream);
        audioChunks = [];
        
        mediaRecorder.addEventListener("dataavailable", event => {
          audioChunks.push(event.data);
        });
        
        mediaRecorder.addEventListener("stop", () => {
          const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });
          const audioUrl = URL.createObjectURL(audioBlob);
          
          // Save voice sample
          voiceSamples.push({
            url: audioUrl,
            blob: audioBlob
          });
          
          // Update samples list
          updateSamplesList();
        });
        
        mediaRecorder.start();
      } catch (error) {
        console.error("Error accessing microphone:", error);
        alert("Unable to access microphone. Please ensure microphone permissions are granted.");
      }
    };
    
    // Stop recording
    const stopRecording = () => {
      if (mediaRecorder && mediaRecorder.state !== "inactive") {
        mediaRecorder.stop();
        mediaRecorder.stream.getTracks().forEach(track => track.stop());
      }
    };
    
    // Update voice samples list
    const updateSamplesList = () => {
      const samplesList = document.querySelector(".samples-list");
      samplesList.innerHTML = "";
      
      voiceSamples.forEach((sample, index) => {
        const sampleItem = document.createElement("div");
        sampleItem.classList.add("sample-item");
        sampleItem.innerHTML = `
          <span>Sample ${index + 1}</span>
          <span class="material-symbols-rounded sample-play" data-index="${index}">play_arrow</span>
        `;
        samplesList.appendChild(sampleItem);
        
        // Add play functionality
        sampleItem.querySelector(".sample-play").addEventListener("click", (e) => {
          const index = e.target.getAttribute("data-index");
          const audio = new Audio(voiceSamples[index].url);
          audio.play();
        });
      });
    };

    // Initialize voices when they are loaded
    speechSynthesis.onvoiceschanged = () => {
      speechSynthesis.getVoices();
    };

    // Initial greeting
    window.addEventListener('DOMContentLoaded', () => {
      setTimeout(() => {
        speakResponse("Hello, I'm Duo Voice Assistant developed by Samartha. Tap the microphone and start speaking to interact with me.");
      }, 1000);
    });
  </script>
</body>
</html>